# CRE
  CRE：A recruitment domain embedding Model. Used for encoding resume or job description texts, serving as the foundation for retrieval, RAG, and Agent.
  CRE：一个招聘领域的嵌入模型。用于对简历或岗位描述文本进行编码，作为检索、RAG（检索增强生成）和智能体（Agent）的基础。

2025/3/28 Released the **CRE0.5.0** model and technical report.
    By means of CNN, a local feature-aware inductive bias is introduced to make local features more prominent in text encoding for human resource scenarios. Specifically, this is an auxiliary fine-tuning method. It improves the encoding quality of the base model by   adding some model parameters for joint training during fine-tuning, and is essentially a projection layer.
    借助CNN，以引入一种局部特征感知的归纳偏好，使在人力资源场景的文本编码中，局部特征更为突出。具体而言，这是一种在辅助微调的方法，通过在微调训练中增加一些模型参数共同训练，从而提高基座模型的编码质量，本质上是一种投影层设计。


2025/6/28 Released the **CRE1.1** model and technical report.
    This paper explores the domain adaptation mechanism of LLM-based Embedding models in recruitment semantic matching tasks. Compared with traditional BERT-like Embedding models, LLM-based Embedding models demonstrate significant advantages in semantic representation, especially adapting to the heterogeneous text alignment needs that may exist between job descriptions and resumes. The findings of this study prove that:
    (1) The effectiveness of the adaptation training paradigm: Using LoRA lightweight fine-tuning combined with domain synthetic data can improve the performance of LLM-based Embedding models in JD2JD, JD2CV, and CV2CV matching tasks;
    (2) A new trend in technological evolution: Through long-context fusion and instruction control, LLM-based Embedding naturally supports multi-granularity semantic parsing in recruitment scenarios (such as capturing the hyponymy relationship of skills), avoiding the structural bottlenecks of traditional models;
    (3) Industrial deployment value: Under the setting of using enhanced query construction in the training phase and directly applying original queries in the testing phase, the model performance remains stable, verifying the robustness and practicality of the method.
    This study confirms that the domain adaptation scheme centered on LoRA + synthetic data provides a reliable path for the engineering implementation of LLM-based Embedding models in complex recruitment semantic matching scenarios.
    本文探究了 LLM-based Embedding 模型在招聘语义匹配任务中的领域适配机制。相较于传统 BERT 类 Embedding 模型，LLM-based Embedding 模型展现出了显著的语义表征优势，尤其适应岗位描述与简历间可能存在的异构文本对齐需求。本研究发现或证明了： (1) 适配训练范式的有效性：采用 LoRA 轻量微调结合领域合成数据，可提升 LLM-based Embedding 模型在 JD2JD、JD2CV、CV2CV 匹配任务上的性能； (2) 技术演进的新趋势：LLM-based Embedding 通过长上下文融合与指令控制，天然支持招聘场景的多粒度语义解析（如技能上下位关系捕捉），避免了传统模型的结构性瓶颈； (3) 工业部署价值：在训练阶段使用增强查询构造、测试阶段直接应用原始查询的设定下，模型性能仍保持稳定，验证了方法的鲁棒性与实用性。 本研究证实：以 LoRA+合成数据为核心的领域适配方案，为 LLM-based Embedding 模型在复杂招聘语义匹配场景中的工程落地提供了可靠路径。


2025/7/13 Released the **CRE-RT strategy** and technical report.
    This paper proposes a novel iterative method—the Refined Thought (RT) scheme, which aims to improve the quality of sentence embeddings by enabling large language models to perform controlled multi-channel information aggregation. The core principles of this scheme include: generating an attention-guided "memory token" from the last hidden layer output of the base embedding model, strategically placing this token to achieve optimal information fusion, and finely managing positional encoding during the iterative process. The new RT scheme is based on existing theories such as cyclic processing, memory enhancement, and attention mechanisms, aiming to promote deeper text understanding. Through extensive experimental verification on various semantic tasks such as PAWSX, STSB, and STS_Algorithm_jd2cv, the results show that the new RT scheme can continuously improve performance, especially on complex tasks, while maintaining the integrity of the base model's representation space. The findings of this study confirm that such principled iterative refinement can generate more robust and higher-quality sentence embeddings, achieving significant progress in text representation learning.
    本文提出了一种新颖的迭代方法——新精炼思想（Refined Thought）方案，旨在 通过使大型语言模型执行受控的多通道信息聚合来提高句子嵌入的质量。该方案的核心原 则包括：从基座 Embedding 模型的 last hidden layer output 中生成一个注意力引导的 “记忆标记”，策略性地放置该标记以实现最佳信息融合，以及在迭代过程中精细管理位置 编码。新 RT 方案以循环处理、记忆增强和注意力机制等既有理论为基础，旨在促进更深 层次的文本理解。通过在 PAWSX、STSB 和 STS_Algorithm_jd2cv 等多种语义任务上的 广泛实验验证，结果表明新 RT 方案能够持续提升性能，尤其是在复杂任务上，同时保持 基座模型表征空间的完整性。本研究结果证实，这种有原则的迭代精炼能够产生更鲁棒、 更高质量的句子嵌入，在文本表征学习方面取得了显著进展。
